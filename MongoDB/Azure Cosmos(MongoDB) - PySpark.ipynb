{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eefedf55-da4c-4268-bd3d-b7a0c36d4c66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+------------+------------------+---------------+----------------+--------------------+------------+------+---------------+-------------+---------+\n|                 _id|company_location|company_size|employee_residence|employment_type|experience_level|           job_title|remote_ratio|salary|salary_currency|salary_in_usd|work_year|\n+--------------------+----------------+------------+------------------+---------------+----------------+--------------------+------------+------+---------------+-------------+---------+\n|{67185ef59ca521f7...|              ES|           L|                ES|             FT|              SE|Principal Data Sc...|         100| 80000|            EUR|        85847|     2023|\n|{67185ef59ca521f7...|              US|           S|                US|             CT|              MI|         ML Engineer|         100| 30000|            USD|        30000|     2023|\n|{67185ef59ca521f7...|              US|           S|                US|             CT|              MI|         ML Engineer|         100| 25500|            USD|        25500|     2023|\n|{67185ef59ca521f7...|              CA|           M|                CA|             FT|              SE|      Data Scientist|         100|175000|            USD|       175000|     2023|\n|{67185ef59ca521f7...|              CA|           M|                CA|             FT|              SE|      Data Scientist|         100|120000|            USD|       120000|     2023|\n+--------------------+----------------+------------+------------------+---------------+----------------+--------------------+------------+------+---------------+-------------+---------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sourceConnectionString = \n",
    "sourceDb = \"demomongodb\"\n",
    "sourceCollection =  \"demomongocollection\"\n",
    "targetConnectionString = \n",
    "targetDb = \"demotarget\"\n",
    "targetCollection =  \"demotgtcollection\"\n",
    "\n",
    "my_spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"myApp\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = my_spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\", sourceConnectionString).option(\"database\", sourceDb).option(\"collection\", sourceCollection).load()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62d6c5af-5825-4306-b712-6a0b4444eb1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"com.mongodb.spark.sql.DefaultSource\").mode(\"append\").option(\"uri\", targetConnectionString).option(\"database\", targetDb).option(\"collection\", targetCollection).option(\"maxBatchSize\", 10).save()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Azure Cosmos(MongoDB) - PySpark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
